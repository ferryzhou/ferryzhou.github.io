---
title: "LLM"
date: 2024-11-30
---

把最近的想法总结一下

第一，感觉它本质上还是一个有一定范化能力的记忆器或者模式匹配器。非常依赖于训练数据。在训练数据以外的知识或者领域是没有智能的。

第二，与传统智能不同的地方在于它是non deterministic 的。有一定随机性。或者说随机性是它的一个feature。会让人感受到创造性。当然另一方面会出现幻觉。

第三，涌现行为有几个原因。第一仍然是海量的数据加一定的范化能力。第二它能同时捕捉到短文本与长文本的上下文模式。通过小的靠谱的模式匹配片段组合到一起能达到复杂的靠谱的效果。CoT就是利用了这一点。小的片段因为空间小因而更容易穷举所有的模式因而更可靠。第三就是随机性。创造本质上就是模式加上变化或是微调。

第四，很像演化算法。从一个状态开始，随机生成很多的下一个状态，根据反馈算权重，根据权重进行采样，聚合到一个新的状态，轮回。

第五，表现出来的推理能力仍然是基于前面提的几个原因。不是真正意义上的推理能力。这一点和人，和程序还是不一样的。最简单的就是数字加法。要解决这个问题是无法通过提升模型来解决的。需要跟程序这种范式结合。

第六，一个人的智能到底是什么？一个人读了很多年的书，学习了很多知识，做了很多的考题，也会解决新的问题。这种融会贯通的能力到底是什么？是问题拆解加记忆匹配？问题拆解本身也是一种记忆匹配吗？

第七，知识是什么？是这个世界这个宇宙运行规则的词语抽象，是人类经验的总结和归纳，是人类创造的新的工具并与世界交互的经验。

第八，从工具论的眼光来看。对于复杂并有精度要求的任务，LLM其实偏底层，类似于互联网最底层的协议，不可靠但也算是基础。它需要结合很多的prompt engineering，结合数据库，结合程序解释器，结合推理，结合规划，结合验证才能真正靠谱的解决复杂的问题。

我现在写这么一段话LLM能生成出来吗？
